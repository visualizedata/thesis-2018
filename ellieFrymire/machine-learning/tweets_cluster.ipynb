{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('oct20_nov01_2.json') as data:\n",
    "#    data = json.load(data)\n",
    "# print (data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets = pd.DataFrame(data)\n",
    "# print(tweets.shape)\n",
    "# print(tweets[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets = pd.DataFrame(columns=['fullname','user','text','timestamp','likes','retweets','replies'])\n",
    "# print (tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nov01_dec01.json', 'jan01_feb01.json', 'dec01_jan01.json', 'oct14_oct20.json', 'oct20_nov01.json', 'feb01_mar01.json', 'oct01_oct14.json']\n"
     ]
    }
   ],
   "source": [
    "path_to_json = \"data/\"\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "print(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file in json_files:\n",
    "    with open('data/' + file) as file:\n",
    "        data.append(json.load(file))\n",
    "#         file = json.load(file)\n",
    "#         print(data.shape)\n",
    "#         merge.append(data)\n",
    "        # print('appending ' + file + ' to tweets ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_files) as data:\n",
    "   data = json.load(data)\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_files = glob.glob(\"data/*.json\")\n",
    "with open(\"merged_file.json\", \"wb\") as outfile:\n",
    "    outfile.write('[{}]'.format(\n",
    "        ','.join([open(f, \"rb\").read() for f in read_files])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(columns=['fullname','user','text','timestamp','likes','retweets','replies'])\n",
    "print (tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js)) as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "\n",
    "        # here you need to know the layout of your json and each json has to have\n",
    "        # the same structure (obviously not the structure I have here)\n",
    "        fullname = json_text['fullname']\n",
    "        text = json_text['text']\n",
    "#         lonlat = json_text['features'][0]['geometry']['coordinates']\n",
    "        # here I push a list of data into a pandas DataFrame at row given by 'index'\n",
    "        jsons_data.loc[index] = [fullname, text]\n",
    "\n",
    "# now that we have the pertinent json data in our DataFrame let's look at it\n",
    "print(jsons_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer()\n",
    "# X_cv = cv.fit_transform(tweets.text)\n",
    "# # cv.get_feature_names() # prints the list of the tokenized words \n",
    "# # X_cv.toarray() # prints the array of each tweet and a 1 if it includes the word in that spot\n",
    "# cv.vocabulary_.get('metoo') # the converse mapping from feature name to column index\n",
    "# # print(X_cv.shape)\n",
    "# # print(X_cv[0:1])\n",
    "\n",
    "# # joblib.dump(cv, 'cv.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bv = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "# X_bv = bv.fit_transform(tweets.text)\n",
    "\n",
    "# # bv.get_feature_names() # prints the list of the tokenized words \n",
    "# # X_bv.toarray() # prints the array of each tweet and a 1 if it includes the word in that spot\n",
    "# bv.vocabulary_.get('metoo') # the converse mapping from feature name to column index\n",
    "# # print(X_bv.shape)\n",
    "# # print(X_bv[0:1])\n",
    "\n",
    "# # joblib.dump(bv, 'bv.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv = HashingVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "# hv\n",
    "X_hv = hv.transform(tweets.text)\n",
    "# print(X_hv.shape)\n",
    "# print(X_hv[0:1])\n",
    "\n",
    "# joblib.dump(hv, 'hv.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "X_tfidf = transformer.fit_transform(X_hv)\n",
    "print(X_tfidf.shape)\n",
    "print(X_tfidf[0:1])\n",
    "\n",
    "joblib.dump(transformer, 'transformer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# terms = tfidf_vectorizer.get_feature_names()\n",
    "# terms = transformer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(cv))\n",
    "print(type(X_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 3\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_kmeans = kmeans.predict(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.labels_.shape)\n",
    "\n",
    "clusters = kmeans.labels_.tolist()\n",
    "print(clusters[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(kmeans, 'kmeans.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['cluster'] = clusters\n",
    "# print(tweets.head())\n",
    "cluster_0 = tweets[tweets.cluster == 0][['fullname','text','timestamp','user']]\n",
    "cluster_1 = tweets[tweets.cluster == 1][['fullname','text','timestamp','user']]\n",
    "cluster_2 = tweets[tweets.cluster == 2][['fullname','text','timestamp','user']]\n",
    "# print(cluster_0.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_0.to_csv('cluster_0')\n",
    "cluster_1.to_csv('cluster_1')\n",
    "cluster_2.to_csv('cluster_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionary = {\n",
    "#     'text': tweets.text, \n",
    "#     'user': tweets.user, \n",
    "#     'timestamp': tweets.timestamp,\n",
    "#     'likes': tweets.likes,\n",
    "#     'cluster': clusters\n",
    "# }\n",
    "# frame = pd.DataFrame(dictionary, \n",
    "#                      index = [clusters] , \n",
    "#                      columns = ['user', 'text', 'timestamp', 'likes', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# frame.loc[frame['cluster'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cluster_1.to_csv('cluster_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grouped = frame['likes'].groupby(frame['cluster'])\n",
    "# print(grouped.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(' %s' % tweets.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "#     print(\"Cluster %d titles:\" % i, end='')\n",
    "#     for user in frame.ix[i]['user'].values.tolist():\n",
    "#         print(' %s,' % user, end='')\n",
    "#     print() #add whitespace\n",
    "#     print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
