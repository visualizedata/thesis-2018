The nature of disease today is quite different than even a century ago. Historically, most major medical issues—such as polio, tuberculosis, and pneumonia—were the result of infectious diseases. These are problems caused by particular microbes—bacteria, viruses, or fungi—that humans could pick up from their environment and then spread. Naturally, the solution to such diseases would be to eliminate the offending microbes, and indeed this is what was done. A major victory in the fight against these diseases was the discovery of penicillin, the first antibiotic. In 1928, Alexander Fleming, a medical researcher, discovered accidentally that a particular species of mold had grown in one of his experiments, and it turned out that this mold produced certain metabolites that were able to kill the bacteria surrounding it. Fleming isolated these bactericidal compounds, which he called penicillin after the name of the mold—with this, medicine had been completely changed. 

Slowly in the past few years, medicine has begun to enter what one might call a post antibiotic era. While theoretical issues of antibiotic resistance have been on the minds of researchers for decades, such issues are only very recently becoming a widespread, practical problem, most notably with rise of certain strains of the bacterium Staphylococcus Aureus that are resistant to numerous families of potent antibiotics. While in the past the solution to this type of issue was to concoct more potent forms of antibiotics, research is now finding that this may not be the best solution. At the crux of this newfound hesitation is the discovery of the importance of the human microbiome—the system of bacteria (and, to a slightly lesser extent, fungi, viruses, and archaea) that live on and inside all humans. Previously thought to be irrelevant to human health, these microbes have now been found to be key to the proper functioning of numerous bodily processes. 

Research on the human microbiome, though becoming more sophisticated everyday, is still quite preliminarModern medicine has become highly skilled in treating infectious disease, so much so that such diseases are hardly a major concern in developed countries. Unfortunately, and consequently, the diseases that plague modern society are chronic in nature, and they are growing ever more prevalent. While medical research is making strides in the management of chronic disease, there is not much emphasis placed on finding and communicating methods of prevention. Prevention of chronic disease is far more favorable than the current state of affairs: while people may be living longer, they are not living as comfortably, often dependent on long-term combination of drugs and procedures.

Ultimately, many chronic diseases are driven by factors of diet and lifestyle—by multiple small elements that accumulate into large effects over time. While both factors are equally important, this thesis is concerned with addressing the changing American diet and food supply as they relate to rates of disease. 

Both food production and food consumption in America have changed drastically in the past century. Over fresh fruits and vegetables, Americans today are more likely to choose processed foods that are artificially fortified with vitamins and minerals. However, this change in consumption is not the only issue: with the advent of fertilizers, pesticides, and industrial agriculture, the nature of American crops has changed, too. The use of fertilizers and pesticides privileges certain nutrients found to be healthful, casting away others; it depletes soil of natural microbial diversity, of microbes that generate key vitamins and minerals as metabolites. Current knowledge of what is healthful or not is limited, and shaping crops to this incomplete knowledge is dangerous. Furthermore, with larger, industrial agriculture, the overall crop distribution in America has shifted, with most farms now producing a single variety of either corn or soy, both easy-to-grow. [[ADD CITATIONS, look up Chicago style inline citations]]

There is opposition to this change, however. With increasing awareness of a changing food supply and unsatisfactory medical care for disease prevention, concepts from natural or alternative healthcare are moving from the fringe to the mainstream though social movements like “Wellness” and “Biohacking.” This is not entirely bad. In fact, such forms of healthcare integrated into society through these movements can have an important perspective to offer. However, there is a great deal of sensationalization that happens in these worlds—statistics are loosely thrown around, reducing nuanced biological processes to single, absolute figures, and pseudoscience abounds. With extensive training in human health, one may be able to sift through this morass of information, but for most, such figures only lead to confusion and, perhaps, anxiety. There is thus very much a need for comprehensible, nuanced visualizations of the available data relating to these topics, and these visualizations must be designed for the non-expert public.

That being said, this project is concerned primarily with deconstructing and communicating the effect of diet on disease in America. Toward that end, it considers both American food consumption and American food production, as well as trends in a particular chronic disease well known to be influenced by diet—diabetes. Tracking this over a span of roughly 100 years, it consists of a timeline exploring the historical changes in the American food supply and how this has correlated with the rate of disease. The primary consideration is the changing nutrient content of American food as a whole, followed by the changing distribution and diversity of American crops. These highly quantitative figures are punctuated by qualitative information, like important innovations in American agriculture (such as the invention and subsequent ban of DDT) and major American diet trends (such as Atkins, or Low Fat). The project superimposes all of this information to create an honest depiction of food and disease in America today and in the past. Avoiding sensationalist statistics and statements, it does not attempt to overly simplify or reduce the complexities that do indeed exist. Rather, curating multiple previously uncombined sources of data and portraying them clearly and plainly, it invites interested readers to discover relationships between them. 
y. Scientists today have some understanding of the diversity of a healthy microbiome and have been able to identify the particular microbes that are most commonly found in humans (as well as any abnormalities for particular individuals and variations between distinct populations). However, they do not understand the precise role of each of these microbes, and it is postulated that they form a complex, interdependent ecosystem. Like any other organisms in an ecosystem, the microbes living on and inside humans compete for survival, existing in an intricate balance that is dependent on the actions of the human host. 

While certain novel therapies are being developed for microbiome manipulation—the Fecal Matter Transplant, a method of transplanting the gut microbiome from a healthy donor to an unhealthy patient through the donor’s fecal matter comes to mind—such therapies are very new, not-widely-tested, and, in the United Sates, limited in approved scope to rather severe diseases. Manipulation of diet thus remains the most practical way of effecting change in the microbiome: certain foods feed certain microbes, so changes in diet affect the distribution of microbes represented. 

This is important because the medical issues that plague society today are largely chronic in nature and are influenced by the state of one’s microbiome. Examples of such issues are autoimmune diseases like Lupus and Rheumatoid Arthritis as well as obesity, diabetes, and the metabolic syndrome. Unlike infectious diseases, these issues cannot be addressed with antimicrobial protocols as they are not caused by a particular offending organism. Rather, they are driven—created and exacerbated—by factors of diet and lifestyle; they arise slowly and are difficult to eradicate. 

Both food habits and lifestyle habits affect human health in two ways: directly and indirectly. There are the direct metabolic effects that have been long acknowledged and are more easily visible: eating too many calories without sufficient exercise, for example, contributes to weight gain, which contributes to the development of diseases like diabetes. However, the indirect effects—physiological effects resulting from a largely invisible change in the internal microbiome—are more insidious and, resultantly, are perhaps more dangerous. Poor dietary habits (and lifestyle habits, interestingly) contribute to a distribution of microbes that produce metabolites that cause the host to further desire such habits. [CITE] This is one—though far from the only—reason changing such habits is difficult, and care should be taken to avoid encouraging the development of poor habits in the first place. With regard to diet in America, the responsibility is then on systems of American food production to make appropriate foods readily available. 

To address lifestyle briefly, data for roughly the past two decades indicates that the physical activity levels of Americans is slowly and steadily increasing; however, rates of obesity and chronic diseases are increasing as well, which is counterintuitive. [CITE] The most logical place, then, to search for why this may be the case is in American food habits. Now, while both diet and lifestyle each have a large impact on the progression of most chronic diseases (whether the relative impact of each is equal or not is still to be determined), fully considering both and their intricacies would be a project of overly large scope with long-term data that is tenuous. For this reason, this project focuses on the effect of food trends and the American food supply on American health, for which there is data that is more robust and stretches a longer timespan. 

While, as mentioned, a great number of diseases are influenced by diet—often through effects on the microbiome, for example—the connection between diet and disease is more clearly evident with certain diseases. One such disease is diabetes, for which there has been a long-acknowledged connection to diet. This acknowledgment, in part, is due to the inherent metabolic nature of diabetes and the resultant directly visible connection to food. For this reason, diabetes is an ideal disease to study in examining the relation between health and food supply. Furthermore, incidence rates of diabetes in America have been tracked by the CDC for over a century, yielding a robust and reliable source of data. 



Both historically and today, Americans on the whole seem to have a predilection for fad dieting. Such dieting practices often vilify a particular macronutrient and praise another or praise a particular way of eating. They are often a result of new research in medical and nutrition science, but the research is often taken out of scientific context and coopted by the public. Resulting diets are often highly absolutist, making strong claims based on research that may be imperfect. Unlike with fully controlled scientific studies, nutrition science involves working with human metabolism, which is a complex system that is not well understood in itself. Nutrition research cannot therefore make absolute claims because all relevant variables may not be considered or fully understood. 

Still, diets based on such research persist, and it is therefore important to see how publication of such research and popularity of such diets affects American food availability and consumption. Towards that end, this section enumerates and elaborates on diets popular in America in the last century. 

Diets are well known for dictating which foods to eat, limiting certain food groups for a desired therapeutic effect; however, certain diets focus more on the methods of food consumption rather than the types of foods consumed themselves. Fletcherism, a diet popular at the beginning of the twentieth century is one such diet. It advocates for being fully guided by one’s appetite: eating whatever is desired, but only when hungry and calm, never when anxious or depressed. Furthermore, it suggests that food should be chewed until it is liquid in the mouth, holding the belief that this would prevent weight gain. 

Following this, around the 1920’s, it became popular to count calories. The true origins of calorie counting can be traced back to the 1890’s with the research of Wilbur O. Atwater at Wesleyan. He and the people working at his lab tested the calorie content of a wide variety of foods and published the results—this was one of the first associations between food and a scientific measure of energy. Extrapolating upon this, Atwater suggested that weight gain could be prevented by not consuming a caloric excess, an excess of energy than what one’s body needs to function. His advice was not fully embraced by the American public, however; that happened with the 1918 publication of “Diet and Health: With Key to the Calories,” a book by physician Lulu Hunt Peters that praised calorie counting. It made counting calories an accessible task by including a list of common foods and their caloric values; it offered a simplified outlook by encouraging readers to view these foods as nothing more than their caloric values, too. This book was the origin of the 1200 calorie diet that the “average” American should follow, which still influences Nutrition Facts labels today. 

As it remains today, calorie counting continued to be the foundation for most health-based eating practices. Newer, “fad” diets added additional rules. One of the earliest such diets that gained popularity was the Hay Diet, named after its founder William Hay, who published his theories in 1929 in a book called “Health via Food.”  Hay categorized most foods into three categories: proteins, carbohydrates (starches), and neutrals. Through his research, he found that digestion of proteins required an acidic digestive environment, while digestion of carbohydrates required an alkaline digestive environment. From this, he concluded that proteins and starches should not be consumed in the same meal; doing so, he thought, would cause a buildup of acid in bodily fluids, which he thought would contribute to the development of diseases like diabetes and certain inflammatory conditions. 